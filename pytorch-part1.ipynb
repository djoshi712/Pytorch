{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Neural networks transform floating-point representations into other floatingpoint representations. The starting and ending representations are typically human interpretable, but the intermediate representations are less so.\n",
    "2. These floating-point representations are stored in tensors.\n",
    "3. Tensors are multidimensional arrays; they are the basic data structure in PyTorch.\n",
    "4. PyTorch has a comprehensive standard library for tensor creation, manipulation, and mathematical operations.\n",
    "5. Tensors can be serialized to disk and loaded back.\n",
    "6. All tensor operations in PyTorch can execute on the CPU as well as on the GPU, with no change in the code.\n",
    "7. PyTorch uses a trailing underscore to indicate that a function operates in place on a tensor (for example, Tensor.sqrt_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "<class 'torch.Tensor'>\n",
      "tensor(1.)\n",
      "1.0\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# 1d array\n",
    "a= torch.ones(5)\n",
    "print(a)\n",
    "print(type(a))\n",
    "print(a[1])\n",
    "print(float(a[1]))\n",
    "# 2d tensor\n",
    "b= torch.ones(2,3)\n",
    "print(b)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python lists or tuples of numbers are collections of Python objects that are individually\n",
    "allocated in memory. PyTorch tensors or NumPy\n",
    "arrays, on the other hand, are views over (typically) contiguous memory blocks containing\n",
    "unboxed C numeric types rather than Python objects. Each element is a 32-bit (4-byte)\n",
    "float in this case. This means storing a 1D\n",
    "tensor of 1,000,000 float numbers will require exactly 4,000,000 contiguous bytes, plus\n",
    "a small overhead for the metadata (such as dimensions and numeric type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4., 1.],\n",
       "         [5., 3.],\n",
       "         [2., 1.]]),\n",
       " torch.Size([3, 2]),\n",
       " tensor(1.),\n",
       " tensor([5., 3.]),\n",
       " tensor([[5., 3.]]),\n",
       " tensor([1., 3.]),\n",
       " tensor([1., 3., 1.]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to represent a geometrical object, perhaps a 2D triangle with vertices at coordinates \n",
    "# (4, 1), (5, 3), and (2, 1).\n",
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "# points[row , column] >> indexing\n",
    "points, points.shape, points[2][1], points[1], points[1:2, ], points[:2, 1], points[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sometimes the RGB channels are in dimension 0, and sometimes they are in dimension 1 But we can generalize by counting from the end: they are always in dimension –3, the third from the end. The lazy, unweighted mean can thus be written as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.1906, -0.3990,  1.4663, -0.6403,  0.5060],\n",
       "          [-0.5317, -0.3363, -0.6422,  1.0012,  0.2621],\n",
       "          [ 1.9996,  1.5415, -0.4395, -0.6198, -0.2690],\n",
       "          [ 0.0649, -0.3438, -0.1805,  0.1883,  0.0440],\n",
       "          [ 0.0092, -1.2074,  0.7788, -1.4720,  0.1654]],\n",
       " \n",
       "         [[-0.0265, -0.9291,  0.3678,  0.6406,  0.5012],\n",
       "          [-0.0472,  0.3932,  0.7769, -0.1731,  0.4117],\n",
       "          [-0.2803,  0.3134, -0.5682, -0.2266,  0.1150],\n",
       "          [-0.3496,  0.6546, -2.0509,  1.4073, -0.3369],\n",
       "          [ 0.2415, -0.3576, -0.4784, -0.8917,  0.9275]],\n",
       " \n",
       "         [[-1.5339,  1.5665, -1.5221,  1.7443,  0.0858],\n",
       "          [-0.1007,  0.7191, -0.2154, -0.1355,  0.0127],\n",
       "          [ 0.1282, -0.5933,  1.1130,  1.0193, -1.4197],\n",
       "          [ 0.8274, -1.6891,  0.1900, -0.7786, -0.7436],\n",
       "          [-0.8988, -1.1767,  0.4368,  1.4466, -0.2988]]]),\n",
       " torch.Size([3, 5, 5]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape [channels, rows, columns]\n",
    "img_t = torch.randn(3, 5, 5)\n",
    "img_t, img_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0515, -0.0494,  0.4426, -0.0454, -0.3452],\n",
      "        [ 0.1108,  0.2723, -0.1293, -0.1351, -0.1118],\n",
      "        [ 0.0681,  0.0561,  0.0495, -0.4388, -0.0982]])\n",
      "tensor([[ 7.0284e-02, -1.4899e-01,  1.9660e-01, -3.0853e-01,  1.4172e-01],\n",
      "        [-9.2398e-02,  1.4908e-02, -3.9057e-01,  1.5130e-01,  3.2370e-01],\n",
      "        [-3.1554e-01, -2.3470e-01,  4.5768e-04,  6.5920e-01, -4.7273e-01]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9170,  0.0795,  0.1040,  0.5815,  0.3643],\n",
       "        [-0.2265,  0.2587, -0.0269,  0.2309,  0.2289],\n",
       "        [ 0.6158,  0.4206,  0.0351,  0.0576, -0.5246],\n",
       "        [ 0.1809, -0.4594, -0.6805,  0.2723, -0.3455],\n",
       "        [-0.2160, -0.9139,  0.2457, -0.3057,  0.2647]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding mean across columns\n",
    "print(img_t.mean(-1))\n",
    "# finding mean across rows\n",
    "print(img_t.mean(-2))\n",
    "# finding mean across channels\n",
    "img_gray_naive = img_t.mean(-3)\n",
    "img_gray_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 1.4061, -0.8478,  0.6290,  1.3703,  1.2419],\n",
       "           [-0.8682, -0.3843,  0.1393, -1.3313,  0.1641],\n",
       "           [ 0.0292,  0.9762, -1.3640, -0.5952, -0.2424],\n",
       "           [ 0.9331,  0.2788, -1.1940, -0.3628,  0.8974],\n",
       "           [-0.0238,  1.9626,  0.1784, -0.0919, -0.2715]],\n",
       " \n",
       "          [[-0.4081,  0.4525,  1.2643,  1.1695, -0.7364],\n",
       "           [-0.3409,  1.8004, -0.8791,  1.7764, -0.0799],\n",
       "           [ 0.7354,  0.5276,  0.6712,  0.7413,  0.1751],\n",
       "           [ 0.0580,  0.2251,  0.8568,  0.3369,  1.4278],\n",
       "           [ 1.2966,  0.8079,  2.0605, -0.7384,  0.3959]],\n",
       " \n",
       "          [[ 0.4270, -0.0847, -0.5829, -0.0885,  0.6286],\n",
       "           [ 0.9648,  1.3463, -0.4546, -0.2526,  1.0077],\n",
       "           [ 0.4707,  0.3930,  0.9050, -0.8490,  0.8095],\n",
       "           [-0.2637,  0.1342,  0.0719,  1.3535,  0.9777],\n",
       "           [-0.6472,  0.1750,  0.6022, -1.0849, -0.5681]]],\n",
       " \n",
       " \n",
       "         [[[-1.5502, -0.8889,  0.6502,  0.5591,  2.7198],\n",
       "           [-0.6045, -1.1649,  0.4860,  0.4917,  1.1709],\n",
       "           [-0.1770,  0.5721, -0.6287, -2.4378, -1.1885],\n",
       "           [-1.1836, -0.2747, -1.5463,  0.2489, -0.3756],\n",
       "           [-1.0742, -0.7051,  0.9307,  0.6468, -1.3105]],\n",
       " \n",
       "          [[ 1.0642, -0.7176,  0.1064,  2.3272,  0.4787],\n",
       "           [-0.5732, -0.3029,  3.1533, -0.6465, -0.2591],\n",
       "           [ 1.1783,  0.5241,  1.8510,  0.2931, -0.2603],\n",
       "           [-0.3710,  0.2743,  1.1851,  1.6531, -1.2677],\n",
       "           [-0.7379,  1.4866,  0.5936, -0.3908, -0.1277]],\n",
       " \n",
       "          [[ 0.1270, -0.9624, -1.2437,  0.7429,  0.3029],\n",
       "           [-0.3655, -0.2708,  0.0032,  0.7535, -0.7317],\n",
       "           [-1.1516,  0.5243,  1.4910,  0.1368,  0.1485],\n",
       "           [ 1.2312, -2.3238,  0.5159, -0.4560, -0.6427],\n",
       "           [-0.7983,  0.8671,  1.5711, -0.1329,  1.6602]]]]),\n",
       " torch.Size([2, 3, 5, 5]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape [batch, channels, rows, columns]\n",
    "batch_t = torch.randn(2, 3, 5, 5)\n",
    "batch_t, batch_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.4750, -0.1600,  0.4368,  0.8171,  0.3780],\n",
       "          [-0.0814,  0.9208, -0.3981,  0.0642,  0.3639],\n",
       "          [ 0.4118,  0.6323,  0.0707, -0.2343,  0.2474],\n",
       "          [ 0.2425,  0.2127, -0.0884,  0.4426,  1.1010],\n",
       "          [ 0.2085,  0.9818,  0.9471, -0.6384, -0.1479]],\n",
       " \n",
       "         [[-0.1196, -0.8563, -0.1624,  1.2097,  1.1671],\n",
       "          [-0.5144, -0.5795,  1.2142,  0.1995,  0.0600],\n",
       "          [-0.0501,  0.5402,  0.9044, -0.6693, -0.4334],\n",
       "          [-0.1078, -0.7747,  0.0516,  0.4820, -0.7620],\n",
       "          [-0.8701,  0.5495,  1.0318,  0.0410,  0.0740]]]),\n",
       " torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch wise mean is computed as\n",
    "batch_gray_naive = batch_t.mean(-3)\n",
    "batch_gray_naive, batch_gray_naive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.1906, -0.3990,  1.4663, -0.6403,  0.5060],\n",
       "          [-0.5317, -0.3363, -0.6422,  1.0012,  0.2621],\n",
       "          [ 1.9996,  1.5415, -0.4395, -0.6198, -0.2690],\n",
       "          [ 0.0649, -0.3438, -0.1805,  0.1883,  0.0440],\n",
       "          [ 0.0092, -1.2074,  0.7788, -1.4720,  0.1654]],\n",
       " \n",
       "         [[-0.0265, -0.9291,  0.3678,  0.6406,  0.5012],\n",
       "          [-0.0472,  0.3932,  0.7769, -0.1731,  0.4117],\n",
       "          [-0.2803,  0.3134, -0.5682, -0.2266,  0.1150],\n",
       "          [-0.3496,  0.6546, -2.0509,  1.4073, -0.3369],\n",
       "          [ 0.2415, -0.3576, -0.4784, -0.8917,  0.9275]],\n",
       " \n",
       "         [[-1.5339,  1.5665, -1.5221,  1.7443,  0.0858],\n",
       "          [-0.1007,  0.7191, -0.2154, -0.1355,  0.0127],\n",
       "          [ 0.1282, -0.5933,  1.1130,  1.0193, -1.4197],\n",
       "          [ 0.8274, -1.6891,  0.1900, -0.7786, -0.7436],\n",
       "          [-0.8988, -1.1767,  0.4368,  1.4466, -0.2988]]]),\n",
       " tensor([0.2126, 0.7152, 0.0722]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tensor([0.2126, 0.7152, 0.0722])\n",
    "img_t, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.2126]],\n",
       " \n",
       "         [[0.7152]],\n",
       " \n",
       "         [[0.0722]]]), torch.Size([3, 1, 1]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsqueezed_weights= weights.unsqueeze(-1).unsqueeze(-1)\n",
    "unsqueezed_weights, unsqueezed_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-2.5312e-01, -8.4829e-02,  3.1173e-01, -1.3612e-01,  1.0757e-01],\n",
       "          [-1.1303e-01, -7.1490e-02, -1.3652e-01,  2.1285e-01,  5.5733e-02],\n",
       "          [ 4.2511e-01,  3.2773e-01, -9.3430e-02, -1.3178e-01, -5.7196e-02],\n",
       "          [ 1.3795e-02, -7.3095e-02, -3.8365e-02,  4.0028e-02,  9.3645e-03],\n",
       "          [ 1.9521e-03, -2.5670e-01,  1.6558e-01, -3.1295e-01,  3.5174e-02]],\n",
       " \n",
       "         [[-1.8934e-02, -6.6450e-01,  2.6308e-01,  4.5815e-01,  3.5847e-01],\n",
       "          [-3.3734e-02,  2.8120e-01,  5.5563e-01, -1.2381e-01,  2.9446e-01],\n",
       "          [-2.0049e-01,  2.2418e-01, -4.0639e-01, -1.6204e-01,  8.2217e-02],\n",
       "          [-2.5000e-01,  4.6819e-01, -1.4668e+00,  1.0065e+00, -2.4093e-01],\n",
       "          [ 1.7275e-01, -2.5576e-01, -3.4218e-01, -6.3777e-01,  6.6333e-01]],\n",
       " \n",
       "         [[-1.1075e-01,  1.1310e-01, -1.0990e-01,  1.2594e-01,  6.1943e-03],\n",
       "          [-7.2680e-03,  5.1916e-02, -1.5551e-02, -9.7826e-03,  9.2023e-04],\n",
       "          [ 9.2594e-03, -4.2833e-02,  8.0359e-02,  7.3592e-02, -1.0250e-01],\n",
       "          [ 5.9741e-02, -1.2196e-01,  1.3718e-02, -5.6216e-02, -5.3688e-02],\n",
       "          [-6.4893e-02, -8.4954e-02,  3.1535e-02,  1.0444e-01, -2.1577e-02]]]),\n",
       " torch.Size([3, 5, 5]),\n",
       " tensor([[-0.3828, -0.6362,  0.4649,  0.4480,  0.4722],\n",
       "         [-0.1540,  0.2616,  0.4036,  0.0793,  0.3511],\n",
       "         [ 0.2339,  0.5091, -0.4195, -0.2202, -0.0775],\n",
       "         [-0.1765,  0.2731, -1.4915,  0.9903, -0.2853],\n",
       "         [ 0.1098, -0.5974, -0.1451, -0.8463,  0.6769]]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_weights = (img_t * unsqueezed_weights)\n",
    "img_weights, img_weights.shape, img_weights.sum(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 2.9893e-01, -1.8024e-01,  1.3373e-01,  2.9133e-01,  2.6403e-01],\n",
       "           [-1.8459e-01, -8.1708e-02,  2.9609e-02, -2.8302e-01,  3.4878e-02],\n",
       "           [ 6.2100e-03,  2.0755e-01, -2.8999e-01, -1.2654e-01, -5.1539e-02],\n",
       "           [ 1.9839e-01,  5.9275e-02, -2.5384e-01, -7.7133e-02,  1.9078e-01],\n",
       "           [-5.0619e-03,  4.1724e-01,  3.7934e-02, -1.9530e-02, -5.7716e-02]],\n",
       " \n",
       "          [[-2.9191e-01,  3.2360e-01,  9.0426e-01,  8.3644e-01, -5.2670e-01],\n",
       "           [-2.4381e-01,  1.2876e+00, -6.2877e-01,  1.2705e+00, -5.7168e-02],\n",
       "           [ 5.2599e-01,  3.7737e-01,  4.8001e-01,  5.3016e-01,  1.2523e-01],\n",
       "           [ 4.1488e-02,  1.6096e-01,  6.1281e-01,  2.4098e-01,  1.0211e+00],\n",
       "           [ 9.2734e-01,  5.7779e-01,  1.4737e+00, -5.2809e-01,  2.8316e-01]],\n",
       " \n",
       "          [[ 3.0831e-02, -6.1126e-03, -4.2083e-02, -6.3915e-03,  4.5383e-02],\n",
       "           [ 6.9660e-02,  9.7199e-02, -3.2819e-02, -1.8239e-02,  7.2754e-02],\n",
       "           [ 3.3986e-02,  2.8377e-02,  6.5344e-02, -6.1300e-02,  5.8448e-02],\n",
       "           [-1.9041e-02,  9.6904e-03,  5.1921e-03,  9.7724e-02,  7.0592e-02],\n",
       "           [-4.6725e-02,  1.2632e-02,  4.3480e-02, -7.8328e-02, -4.1020e-02]]],\n",
       " \n",
       " \n",
       "         [[[-3.2958e-01, -1.8898e-01,  1.3824e-01,  1.1886e-01,  5.7824e-01],\n",
       "           [-1.2852e-01, -2.4765e-01,  1.0332e-01,  1.0453e-01,  2.4894e-01],\n",
       "           [-3.7632e-02,  1.2164e-01, -1.3367e-01, -5.1827e-01, -2.5266e-01],\n",
       "           [-2.5164e-01, -5.8396e-02, -3.2874e-01,  5.2909e-02, -7.9855e-02],\n",
       "           [-2.2837e-01, -1.4991e-01,  1.9787e-01,  1.3751e-01, -2.7861e-01]],\n",
       " \n",
       "          [[ 7.6115e-01, -5.1320e-01,  7.6077e-02,  1.6644e+00,  3.4238e-01],\n",
       "           [-4.0992e-01, -2.1666e-01,  2.2553e+00, -4.6240e-01, -1.8534e-01],\n",
       "           [ 8.4274e-01,  3.7483e-01,  1.3238e+00,  2.0961e-01, -1.8616e-01],\n",
       "           [-2.6531e-01,  1.9615e-01,  8.4760e-01,  1.1823e+00, -9.0666e-01],\n",
       "           [-5.2776e-01,  1.0632e+00,  4.2455e-01, -2.7948e-01, -9.1337e-02]],\n",
       " \n",
       "          [[ 9.1715e-03, -6.9488e-02, -8.9792e-02,  5.3634e-02,  2.1868e-02],\n",
       "           [-2.6390e-02, -1.9549e-02,  2.3407e-04,  5.4402e-02, -5.2830e-02],\n",
       "           [-8.3146e-02,  3.7857e-02,  1.0765e-01,  9.8784e-03,  1.0720e-02],\n",
       "           [ 8.8891e-02, -1.6778e-01,  3.7248e-02, -3.2924e-02, -4.6403e-02],\n",
       "           [-5.7637e-02,  6.2606e-02,  1.1343e-01, -9.5969e-03,  1.1987e-01]]]]),\n",
       " torch.Size([2, 3, 5, 5]),\n",
       " tensor([[[ 0.0379,  0.1372,  0.9959,  1.1214, -0.2173],\n",
       "          [-0.3587,  1.3031, -0.6320,  0.9692,  0.0505],\n",
       "          [ 0.5662,  0.6133,  0.2554,  0.3423,  0.1321],\n",
       "          [ 0.2208,  0.2299,  0.3642,  0.2616,  1.2825],\n",
       "          [ 0.8756,  1.0077,  1.5551, -0.6259,  0.1844]],\n",
       " \n",
       "         [[ 0.4407, -0.7717,  0.1245,  1.8369,  0.9425],\n",
       "          [-0.5648, -0.4839,  2.3588, -0.3035,  0.0108],\n",
       "          [ 0.7220,  0.5343,  1.2978, -0.2988, -0.4281],\n",
       "          [-0.4281, -0.0300,  0.5561,  1.2023, -1.0329],\n",
       "          [-0.8138,  0.9759,  0.7358, -0.1516, -0.2501]]]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_weights = (batch_t * unsqueezed_weights)\n",
    "batch_weights, batch_weights.shape, batch_weights.sum(-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python objects**\n",
    "1. Numbers in Python are objects. Whereas a floating-point number might require only, for instance, 32 bits to be represented on a computer, Python will convert it into a full-fledged Python object with reference counting, and so on. This operation, called boxing, is not a problem if we need to store a small number of numbers, but allocating millions gets very inefficient.\n",
    "2. Lists in Python are meant for sequential collections of objects. There are no operations defined for, say, efficiently taking the dot product of two vectors, or summing vectors together. Also, Python lists have no way of optimizing the layout of their contents in memory, as they are indexable collections of pointers to Python objects (of any kind, not just numbers). Finally, Python lists are one-dimensional, and although we can create lists of lists, this is again very inefficient.\n",
    "3. The Python interpreter is slow compared to optimized, compiled code. Performing mathematical operations on large collections of numerical data can be much faster using optimized code written in a compiled, low-level language like C.\n",
    "\n",
    "For these reasons, data science libraries rely on NumPy or introduce dedicated data\n",
    "structures like PyTorch tensors, which provide efficient low-level implementations of\n",
    "numerical data structures and related operations on them, wrapped in a convenient\n",
    "high-level API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***torch.transpose(input, dim0, dim1) → Tensor***\n",
    "Returns a tensor that is a transposed version of input. The given dimensions dim0 and dim1 are swapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3, 2)\n",
    "print(a)\n",
    "a_t = torch.transpose(a, 0, 1)\n",
    "a_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or\n",
    "a_t = a.transpose(0, 1)\n",
    "a_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensors: Scenic views of storage**\n",
    "Values in\n",
    "tensors are allocated in contiguous chunks of memory managed by torch.Storage\n",
    "instances. A storage is a one-dimensional array of numerical data: that is, a contiguous\n",
    "block of memory containing numbers of a given type, such as float (32 bits representing\n",
    "a floating-point number) or int64 (64 bits representing an integer).\n",
    "\n",
    "Even though the tensor reports itself as having three rows and two columns, the storage\n",
    "under the hood is a contiguous array of size 6. In this sense, the tensor just knows\n",
    "how to translate a pair of indices into a location in the storage.\n",
    "\n",
    "We can’t index a storage of a 2D tensor using two indices. The layout of a storage is\n",
    "always one-dimensional, regardless of the dimensionality of any and all tensors that\n",
    "might refer to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 3.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " 4.0\n",
       " 1.0\n",
       " 5.0\n",
       " 3.0\n",
       " 2.0\n",
       " 1.0\n",
       "[torch.FloatStorage of size 6]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "print(points[1])\n",
    "points.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_storage = points.storage()\n",
    "points_storage[3]\n",
    "# or points.storage[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-1e957c860944>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpoints\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# We can’t index a storage of a 2D tensor using two indices. The layout of a storage is always one-dimensional, regardless of the dimensionality of any and all tensors that might refer to it\n",
    "points.storage[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modifying stored values: In-place operations** <br>\n",
    "zero_, which indicates that the\n",
    "method operates in place by modifying the input instead of creating a new output tensor\n",
    "and returning it. For instance, the zero_ method zeros out all the elements of the input.\n",
    "Any method without the trailing underscore leaves the source tensor unchanged and\n",
    "instead returns a new tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.ones(3,2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor metadata: Size, offset, and stride**<br>\n",
    "1. The size (or shape, in NumPy parlance) is a tuple indicating how many elements across each dimension the tensor represents. It’s important to note that this is the same information contained in the shape property of tensor objects:\n",
    "2. The storage offset is the index in the storage corresponding to the first element in the tensor.\n",
    "3. The stride is the number of elements in the storage that need to be skipped over to obtain the next element along each dimension.\n",
    "4. Accessing an element i, j in a 2D tensor results in accessing the storage_offset + stride[0] * i + stride[1] * j element in the storage. The offset will usually be zero; if this tensor is a view of a storage created to hold a larger tensor, the offset might be a positive value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 3.]) torch.Size([3, 2]) torch.Size([3, 2]) (2, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "print(points[1], points.shape, points.size(), points.stride())\n",
    "second_point = points[1]\n",
    "second_point.storage_offset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transposing**<br>\n",
    "This tells us that increasing the first index by one in points—for example, going from\n",
    "points[0,0] to points[1,0]—will skip along the storage by two elements, while increasing\n",
    "the second index—from points[0,0] to points[0,1]—will skip along the storage by\n",
    "one. In other words, the storage holds the elements in the tensor sequentially row by row.<br>\n",
    "**Transposing in higher dimensions**<br>\n",
    "We can transpose a multidimensional array by specifying the two dimensions along which transposing\n",
    "(flipping shape and stride) should occur:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5., 2.],\n",
       "        [1., 3., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_t = points.t()\n",
    "points_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(points.storage()) == id(points_t.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 1), (1, 2))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.stride(), points_t.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4, 5]), torch.Size([5, 4, 3]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_t = torch.randn(3, 4, 5)\n",
    "transpose_t = some_t.transpose(0, 2)\n",
    "some_t.shape, transpose_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0326, -1.3739,  0.6134,  0.5517, -0.0641],\n",
       "          [ 0.2173, -1.0482,  0.1910, -1.4370,  1.4592],\n",
       "          [-1.4582,  0.6801,  1.2730, -0.2057,  0.6537],\n",
       "          [-0.8117,  0.3929,  0.0356,  1.1638,  0.8348]],\n",
       " \n",
       "         [[ 1.0885,  1.4824, -0.0166,  1.0396, -0.4837],\n",
       "          [ 0.2324,  0.2062, -0.4196,  0.5573, -0.4953],\n",
       "          [ 0.5752, -2.7219,  0.0399,  0.7238, -1.3694],\n",
       "          [ 0.5891,  0.6332, -1.2489,  1.7633,  0.4609]],\n",
       " \n",
       "         [[ 0.2144,  0.8151, -0.1596, -1.6279,  1.6376],\n",
       "          [-0.5642, -1.2167, -1.9270, -0.9142, -0.5977],\n",
       "          [ 0.2281, -2.2899,  0.6417, -0.9800, -0.5992],\n",
       "          [ 1.2387,  1.0144,  0.5934, -0.2347, -0.3119]]]),\n",
       " tensor([[[ 0.0326,  1.0885,  0.2144],\n",
       "          [ 0.2173,  0.2324, -0.5642],\n",
       "          [-1.4582,  0.5752,  0.2281],\n",
       "          [-0.8117,  0.5891,  1.2387]],\n",
       " \n",
       "         [[-1.3739,  1.4824,  0.8151],\n",
       "          [-1.0482,  0.2062, -1.2167],\n",
       "          [ 0.6801, -2.7219, -2.2899],\n",
       "          [ 0.3929,  0.6332,  1.0144]],\n",
       " \n",
       "         [[ 0.6134, -0.0166, -0.1596],\n",
       "          [ 0.1910, -0.4196, -1.9270],\n",
       "          [ 1.2730,  0.0399,  0.6417],\n",
       "          [ 0.0356, -1.2489,  0.5934]],\n",
       " \n",
       "         [[ 0.5517,  1.0396, -1.6279],\n",
       "          [-1.4370,  0.5573, -0.9142],\n",
       "          [-0.2057,  0.7238, -0.9800],\n",
       "          [ 1.1638,  1.7633, -0.2347]],\n",
       " \n",
       "         [[-0.0641, -0.4837,  1.6376],\n",
       "          [ 1.4592, -0.4953, -0.5977],\n",
       "          [ 0.6537, -1.3694, -0.5992],\n",
       "          [ 0.8348,  0.4609, -0.3119]]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_t, transpose_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Moving tensors to the GPU**\n",
    "1. PyTorch tensors also can be stored on a different kind of processor: a graphics processing unit (GPU). Every PyTorch tensor can be transferred to (one of) the GPU(s) in order to perform massively parallel, fast computations.\n",
    "2. a PyTorch Tensor also has the notion of device, which is where on the computer the tensor data is placed.\n",
    "3. Doing so returns a new tensor that has the same numerical data, but stored in the RAM of the GPU, rather than in regular system RAM. Now that the data is stored locally on the GPU, we’ll start to see the speedups mentioned earlier when performing mathematical operations on the tensor. In almost all cases, CPU- and GPU-based tensors expose the same user-facing API, making it much easier to write code that is agnostic to where, exactly, the heavy number crunching is running.\n",
    "4. If our machine has more than one GPU, we can also decide on which GPU we allocate the tensor by passing a zero-based integer identifying the GPU on the machine,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-112932d857f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# error because i'm executing the codes on a CPU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpoints_gpu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m5.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py36\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    160\u001b[0m         raise RuntimeError(\n\u001b[0;32m    161\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m--> 162\u001b[1;33m     \u001b[0m_check_driver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[0m_cudart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py36\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[1;34m()\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_check_driver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_isDriverSufficient'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_isDriverSufficient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_getDriverVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# error because i'm executing the codes on a CPU \n",
    "points_gpu = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]], device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-13a184aec228>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# We could instead copy a tensor created on the CPU onto the GPU using the to method:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpoints_gpu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpoints\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py36\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    160\u001b[0m         raise RuntimeError(\n\u001b[0;32m    161\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m--> 162\u001b[1;33m     \u001b[0m_check_driver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[0m_cudart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py36\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[1;34m()\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_check_driver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_isDriverSufficient'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_isDriverSufficient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_getDriverVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# We could instead copy a tensor created on the CPU onto the GPU using the to method:\n",
    "points_gpu = points.to(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-940a569fd60d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpoints_gpu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpoints\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cuda:0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py36\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    160\u001b[0m         raise RuntimeError(\n\u001b[0;32m    161\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m--> 162\u001b[1;33m     \u001b[0m_check_driver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[0m_cudart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py36\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[1;34m()\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_check_driver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_isDriverSufficient'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_isDriverSufficient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_getDriverVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "points_gpu = points.to(device='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Serializing tensors- saving data to file**<br>\n",
    "Creating a tensor on the fly is all well and good, but if the data inside is valuable, we will\n",
    "want to save it to a file and load it back at some point. After all, we don’t want to have\n",
    "to retrain a model from scratch every time we start running our program!\n",
    "PyTorch uses **pickle** under the hood to serialize the tensor object, plus dedicated serialization code\n",
    "for the storage. Here’s how we can save our points tensor to an ourpoints.t file.<br><br>\n",
    "While we can quickly save tensors this way if we only want to load them with PyTorch,\n",
    "the file format itself is not interoperable: we can’t read the tensor with software other\n",
    "than PyTorch. Depending on the use case, this may or may not be a limitation, but we\n",
    "should learn how to save tensors interoperably for those times when it is.<br><br>\n",
    "**Serializing to HDF5 with h5py**<br>\n",
    "HDF5 is a portable, widely supported\n",
    "format for representing serialized multidimensional arrays, organized in a nested keyvalue\n",
    "dictionary. Python supports HDF5 through the h5py library (www.h5py.org),\n",
    "which accepts and returns data in the form of NumPy arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 1.],\n",
      "        [5., 3.],\n",
      "        [2., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(points)\n",
    "torch.save(points, 'D:/Research/Pytorch/points.t')\n",
    "# As an alternative, we can pass a file descriptor in lieu of the filename\n",
    "# with open('D:/Research/Pytorch/points.t','wb') as f:\n",
    "#     torch.save(points, f)\n",
    "# Loading our points back is similarly a one-liner\n",
    "points = torch.load('D:/Research/Pytorch/points.t')\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 1.],\n",
      "        [5., 3.],\n",
      "        [2., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# $ conda install h5py\n",
    "import h5py\n",
    "print(points)\n",
    "f = h5py.File('D:/Research/Pytorch/points.hdf5', 'w')\n",
    "dset = f.create_dataset('key_name', data=points.numpy())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here 'coords' is a key into the HDF5 file. We can have other keys—even nested ones.\n",
    "# One of the interesting things in HDF5 is that we can index the dataset while on disk\n",
    "# and access only the elements we’re interested in. Let’s suppose we want to load just\n",
    "# the last two points in our dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 1.] <class 'numpy.ndarray'>\n",
      "tensor([2., 1.]) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File('D:/Research/Pytorch/points.hdf5', 'r')\n",
    "k=f['key_name']\n",
    "last_points=k[-1]\n",
    "print(last_points, type(last_points))\n",
    "# convert last_points to torch data type\n",
    "last_points = torch.from_numpy(last_points)\n",
    "print(last_points, type(last_points))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
